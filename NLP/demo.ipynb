{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joeva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joeva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words\n",
    "# from nltk.corpus import stopwords\n",
    "# stop=stopwords.words('english')\n",
    "# print(stop)\n",
    "# print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "sentence='luminar technolab is IT finishing school located at kakkanad'\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# words=word_tokenize(sentence)\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.casual.TweetTokenizer object at 0x000001878FE6EC60>\n"
     ]
    }
   ],
   "source": [
    "# tweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "words=TweetTokenizer(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'toolkit',\n",
       " '(',\n",
       " 'nltk',\n",
       " ')',\n",
       " 'open',\n",
       " 'source',\n",
       " 'python',\n",
       " 'libary',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1='''The Natural Language Toolkit (NLTK) is an open source Python libary for Natural Language Processing'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "sw=stopwords.words('english')\n",
    "tokens=word_tokenize(sentence1.lower())\n",
    "remove_stop=[i for i in tokens if i not in sw]  # .lower() coverts all words in token to lowercase\n",
    "remove_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'Natural', 'Language')\n",
      "('Natural', 'Language', 'Toolkit')\n",
      "('Language', 'Toolkit', '(')\n",
      "('Toolkit', '(', 'NLTK')\n",
      "('(', 'NLTK', ')')\n",
      "('NLTK', ')', 'is')\n",
      "(')', 'is', 'an')\n",
      "('is', 'an', 'open')\n",
      "('an', 'open', 'source')\n",
      "('open', 'source', 'Python')\n",
      "('source', 'Python', 'libary')\n",
      "('Python', 'libary', 'for')\n",
      "('libary', 'for', 'Natural')\n",
      "('for', 'Natural', 'Language')\n",
      "('Natural', 'Language', 'Processing')\n"
     ]
    }
   ],
   "source": [
    "# N-gram\n",
    "from nltk.util import ngrams\n",
    "gram=ngrams(sequence=nltk.word_tokenize(sentence1),n=3)\n",
    "for i in gram:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking : walk\n",
      "running : run\n",
      "eating : eat\n",
      "waiting : wait\n",
      "thought : thought\n",
      "written : written\n",
      "wrote : wrote\n",
      "writing : write\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "#porterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "words=['walking','running','eating','waiting','thought','written','wrote','writing']\n",
    "for i in words:\n",
    "    print(i,':',ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking : walk\n",
      "running : run\n",
      "eating : eat\n",
      "waiting : wait\n",
      "thought : thought\n",
      "written : written\n",
      "wrote : wrote\n",
      "writing : write\n"
     ]
    }
   ],
   "source": [
    "# SnowBallStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "st=SnowballStemmer('english')\n",
    "for i in words:\n",
    "    print(i,':',st.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "programs : program\n",
      "eating : eating\n",
      "reaches : reach\n"
     ]
    }
   ],
   "source": [
    "#Lemematization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "lem=WordNetLemmatizer()\n",
    "print('rocks',\":\",lem.lemmatize('rocks'))\n",
    "print('programs',\":\",lem.lemmatize('programs'))\n",
    "print('eating',\":\",lem.lemmatize('eating'))\n",
    "print('reaches',\":\",lem.lemmatize('reaches'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "\n",
    "#re\n",
    "#Regular Expression\n",
    "#Pattern Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " onderful_@ honny@123#!)!!~\n",
      "W        _@J     @123#!)!!~\n",
      "Wonderful_@Jhonny@   #!)!!~\n",
      "_@@#!)!!~\n",
      "WonderfulJhonny123\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "str1='Wonderful_@Jhonny@123#!)!!~'\n",
    "str2=re.sub('[A-Z]',' ',str1)#[A-Z]\n",
    "print(str2)\n",
    "str2=re.sub('[a-z]',' ',str1)#[a-z]\n",
    "print(str2)\n",
    "str2=re.sub('[0-9]',' ',str1)#[0-9]\n",
    "print(str2)\n",
    "# Removing multiple different character together\n",
    "str2=re.sub('[A-Za-z0-9]','',str1)#[A-Za-z0-9]\n",
    "print(str2)\n",
    "str2=re.sub('[^A-Za-z0-9]','',str1)#[^A-Za-z0-9] ====>   ^ : cap symbol is used as negation \n",
    "print(str2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
